//Author(s)
//NClark
//IDEXX Laboratories, Inc.
//Westbrook, ME 04092

//Tool Breaks Down Data from Beckman Wet Chemistry Analyzers (models 680, 5800)

//------------------Revision History for both script and Help File--
//
//	1.2.0 NClark 042922
//		-> ----- NOTE STILL NEED TO MAKE WORK CORRECTLY FOR XLSX AND MODEL 680 -----
//		-> updated newCap() to resolve IDE issues with font formatting (didn't seem to cause JMP processing errors)
//		-> updated obtainFileInfo()
//			-> to use filepath instead of a list and iterator
//			-> updated to read the 1st 50 lines of data
//			-> added code to identify  modelFam = DxC700 in addition to 680 and 5800
//			-> updated to correctly obtain indexPieces, indexFileName, and serialNum when modelFam = DxC700
//		-> updated openCSV() to use filePath instead of list and interator
//		-> updated beckmanCleanup()
//			-> to use tableName intead of list and iterator
//			-> added try() to create Unit No. data for DxC700 and probably for 680, but not duplicate if already exists
//			-> removed the tracking of the iterator for failures (not sure if that's actually needed anymore)
//		-> updated fileProcessing()
//			-> removed the open XLS match (although will need to add back in)
//			-> updated use of beckmanCleanup() for parameter updates above
//			-> moved the closing of the intermediate tables into fileProcessing() rather than in finalProcessing()
//		-> updated finalProcessing()
//			-> coloring of columns is now based on a list of columns, and each table will work through the list and color accordingly (old way listed each column separately for each table)
//			-> deleted the code for closing intermediate tables as that was aded to fileProcessing()
//		-> updated MAIN
//			-> Throw() replaced with Stop() in the beckfiles IF statement
//			-> updated print statement to reference the beckman script
//	1.1.0.0 NClark 032720
//		-> beckman cleanup
//			-> renamed uniquerunid uniquecurveid
//			-> created uniquerunid (original uniquerunid without wavelength)
//		-> flile processing
//			-> added uniquecurveid after uniquerunid in col list to move
//		-> final processing
//			-> added Make just Results button to progCurves file
//	1.0.0.1 NClark 020320
//		-> Fixed Show for var that didn't exist anymore
//	1.0.0.0 NClark 020220
//		-> 1st Generic Version Released
//------------------------------------------------------------------

Names Default to Here(1);

//-----------Variable Declarations (above Main for easy access)--
	
	u = Get Environment Variable("username");
	addinID = "com.idexx.generic_beckman";

	updateIDs = {"com.idexx.assayDevLibraries2","com.idexx.generic_beckman"};

	minJMPver = "14";
	
	fileProcessingMsg = "Now Processing File\!n\!n";	
		
	//Sets up MMDDYYYY date for Table Naming
	todayDate=Short Date( Today() );
	newTodayDate = Munger(todayDate,1,"/","");
	nextTodayDate = Munger(newTodayDate,1,"/","");

//-----------/Variable Declarations-----------------------

Include( "$ADDIN_HOME(com.idexx.assayDevLibraries2)\AssayDev_Utilities.jsl" );

checkVersion = Function( {minVer},
	{default local},
	version = Num( Substitute( minVer, ".", "" ) );
	If( Num( Word( 1, JMP Version(), "." ) ) < version,
		a = 0,
		a = 1
	);
	Return( a );
);

newCap = Function({message,delayBool=1},{Default Local},
	Caption(
		{500, 500},
		message,
		Font( "Arial" ),
		Font Size( 16 ),
		Text Color( "Blue" ),
		Back Color( "Yellow" ),
		//Spoken( 1 ),
		//Delayed( 1 )
	);
	If(delayBool == 1,wait(0.5),Empty());
);

beckmanColumnArray = Associative Array(
	{
		{"Bottle No.", {"Numeric", "Continuous"}},
		{"Bottle No. 2", {"Character", "Nominal"}},
		{"Bottle No. 3", {"Numeric", "Continuous"}},
		{"Cal/QC No.", {"Character", "Nominal"}},
		{"Calibration", {"Character", "Nominal"}},
		{"Column 16", {"Character", "Nominal"}},
		{"Column 60", {"Character", "Nominal"}},
		{"Conc.", {"Character", "Nominal"}},
		{"Counts", {"Character", "Nominal"}},
		{"Cuvette", {"Character", "Nominal"}},
		{"Data Flags", {"Character", "Nominal"}},
		{"Measure Time", {"Numeric", "Continuous"}},
		{"P0", {"Character", "Nominal"}},
		{"P1", {"Character", "Nominal"}},
		{"P10", {"Character", "Nominal"}},
		{"P11", {"Character", "Nominal"}},
		{"P12", {"Character", "Nominal"}},
		{"P13", {"Character", "Nominal"}},
		{"P14", {"Character", "Nominal"}},
		{"P15", {"Character", "Nominal"}},
		{"P16", {"Character", "Nominal"}},
		{"P17", {"Character", "Nominal"}},
		{"P18", {"Character", "Nominal"}},
		{"P19", {"Character", "Nominal"}},
		{"P2", {"Character", "Nominal"}},
		{"P20", {"Character", "Nominal"}},
		{"P21", {"Character", "Nominal"}},
		{"P22", {"Character", "Nominal"}},
		{"P23", {"Character", "Nominal"}},
		{"P24", {"Character", "Nominal"}},
		{"P25", {"Character", "Nominal"}},
		{"P26", {"Character", "Nominal"}},
		{"P27", {"Character", "Nominal"}},
		{"P3", {"Character", "Nominal"}},
		{"P4", {"Character", "Nominal"}},
		{"P5", {"Character", "Nominal"}},
		{"P6", {"Character", "Nominal"}},
		{"P7", {"Character", "Nominal"}},
		{"P8", {"Character","Nominal"}},
		{"P9", {"Character", "Nominal"}},
		{"Photocal", {"Numeric", "Continuous"}},
		{"Photocal 2", {"Numeric", "Continuous"}},
		{"Pos.", {"Numeric", "Continuous"}},
		{"Pos. 2", {"Character", "Nominal"}},
		{"Pos. 3", {"Numeric", "Continuous"}},
		{"Preprocess MixBarNo.", {"Character","Nominal"}},
		{"R1 MixBarNo.", {"Numeric", "Continuous"}},
		{"R1(R1-1) Lot No.", {"Numeric", "Continuous"}},
		{"R1-2 Lot No.", {"Character", "Nominal"}},
		{"R2 MixBarNo.", {"Numeric", "Continuous"}},
		{"R2(R2-1) Lot No.", {"Numeric", "Continuous"}},
		{"Rack No.", {"Numeric", "Continuous"}},
		{"RB", {"Character", "Nominal"}},
		{"Reaction OD", {"Numeric", "Continuous"}},
		{"S. MixBarNo.", {"Numeric", "Continuous"}},
		{"S. No.", {"Character","Nominal"}},
		{"Seq. No.", {"Character", "Nominal"}},
		{"Test Name", {"Character", "Nominal"}},
		{"Unit No.", {"Numeric", "Continuous"}},
		{"CONC", {"Numeric", "Continuous"}},
		{"Cuvette No.", {"Character", "Nominal"}},
		{"DataFlag", {"Character", "Nominal"}},
		{"Measured Date/Time", {"Numeric","Continuous"}},
		{"Mix Bar No.", {"Character", "Nominal"}},
		{"Pre.P0", {"Numeric", "Continuous"}},
		{"Pre.P1", {"Numeric", "Continuous"}},
		{"Pre.P10", {"Numeric", "Continuous"}},
		{"Pre.P11", {"Numeric", "Continuous"}},
		{"Pre.P12", {"Numeric", "Continuous"}},
		{"Pre.P13", {"Numeric", "Continuous"}},
		{"Pre.P14", {"Numeric", "Continuous"}},
		{"Pre.P15", {"Numeric", "Continuous"}},
		{"Pre.P16", {"Numeric", "Continuous"}},
		{"Pre.P17", {"Numeric", "Continuous"}},
		{"Pre.P18", {"Numeric", "Continuous"}},
		{"Pre.P19", {"Numeric", "Continuous"}},
		{"Pre.P2", {"Numeric", "Continuous"}},
		{"Pre.P20", {"Numeric", "Continuous"}},
		{"Pre.P21", {"Numeric", "Continuous"}},
		{"Pre.P22", {"Numeric", "Continuous"}},
		{"Pre.P23", {"Numeric", "Continuous"}},
		{"Pre.P24", {"Numeric", "Continuous"}},
		{"Pre.P25", {"Numeric", "Continuous"}},
		{"Pre.P26", {"Numeric", "Continuous"}},
		{"Pre.P27", {"Numeric","Continuous"}},
		{"Pre.P3", {"Numeric", "Continuous"}},
		{"Pre.P4", {"Numeric", "Continuous"}},
		{"Pre.P5", {"Numeric", "Continuous"}},
		{"Pre.P6", {"Numeric", "Continuous"}},
		{"Pre.P7", {"Numeric", "Continuous"}},
		{"Pre.P8", {"Numeric", "Continuous"}},
		{"Pre.P9", {"Numeric", "Continuous"}},
		{"QC/CalNo.", {"Character", "Nominal"}},
		{"RB.P0", {"Numeric", "Continuous"}},
		{"RB.P1", {"Numeric", "Continuous"}},
		{"RB.P10", {"Numeric","Continuous"}},
		{"RB.P11", {"Numeric", "Continuous"}},
		{"RB.P12", {"Numeric", "Continuous"}},
		{"RB.P13", {"Numeric", "Continuous"}},
		{"RB.P14", {"Numeric", "Continuous"}},
		{"RB.P15", {"Numeric", "Continuous"}},
		{"RB.P16", {"Numeric", "Continuous"}},
		{"RB.P17", {"Numeric", "Continuous"}},
		{"RB.P18", {"Numeric", "Continuous"}},
		{"RB.P19", {"Numeric", "Continuous"}},
		{"RB.P2", {"Numeric", "Continuous"}},
		{"RB.P20", {"Numeric","Continuous"}},
		{"RB.P21", {"Numeric", "Continuous"}},
		{"RB.P22", {"Numeric", "Continuous"}},
		{"RB.P23", {"Numeric", "Continuous"}},
		{"RB.P24", {"Numeric", "Continuous"}},
		{"RB.P25", {"Numeric", "Continuous"}},
		{"RB.P26", {"Numeric", "Continuous"}},
		{"RB.P27", {"Numeric", "Continuous"}},
		{"RB.P3", {"Numeric", "Continuous"}},
		{"RB.P4", {"Numeric", "Continuous"}},
		{"RB.P5", {"Numeric", "Continuous"}},
		{"RB.P6", {"Numeric", "Continuous"}},
		{"RB.P7", {"Numeric", "Continuous"}},
		{"RB.P8", {"Numeric", "Continuous"}},
		{"RB.P9", {"Numeric", "Continuous"}},
		{"Rea.P0", {"Numeric","Continuous"}},
		{"Rea.P1", {"Numeric", "Continuous"}},
		{"Rea.P10", {"Numeric", "Continuous"}},
		{"Rea.P11", {"Numeric", "Continuous"}},
		{"Rea.P12", {"Numeric", "Continuous"}},
		{"Rea.P13", {"Numeric", "Continuous"}},
		{"Rea.P14", {"Numeric", "Continuous"}},
		{"Rea.P15", {"Numeric", "Continuous"}},
		{"Rea.P16", {"Numeric", "Continuous"}},
		{"Rea.P17", {"Numeric", "Continuous"}},
		{"Rea.P18", {"Numeric", "Continuous"}},
		{"Rea.P19", {"Numeric","Continuous"}},
		{"Rea.P2", {"Numeric", "Continuous"}},
		{"Rea.P20", {"Numeric", "Continuous"}},
		{"Rea.P21", {"Numeric", "Continuous"}},
		{"Rea.P22", {"Numeric", "Continuous"}},
		{"Rea.P23", {"Numeric", "Continuous"}},
		{"Rea.P24", {"Numeric", "Continuous"}},
		{"Rea.P25", {"Numeric", "Continuous"}},
		{"Rea.P26", {"Numeric", "Continuous"}},
		{"Rea.P27", {"Numeric", "Continuous"}},
		{"Rea.P3", {"Numeric", "Continuous"}},
		{"Rea.P4", {"Numeric","Continuous"}},
		{"Rea.P5", {"Numeric", "Continuous"}},
		{"Rea.P6", {"Numeric", "Continuous"}},
		{"Rea.P7", {"Numeric", "Continuous"}},
		{"Rea.P8", {"Numeric", "Continuous"}},
		{"Rea.P9", {"Numeric", "Continuous"}},
		{"sample ID_data", {"Character", "Nominal"}},
		{"Sample Kind", {"Character","Nominal"}},
		{"Sample No.", {"Character", "Nominal"}},
		{"Sec.P0", {"Character", "Nominal"}},
		{"Sec.P1", {"Character", "Nominal"}},
		{"Sec.P10", {"Character", "Nominal"}},
		{"Sec.P11", {"Character", "Nominal"}},
		{"Sec.P12", {"Character", "Nominal"}},
		{"Sec.P13", {"Character", "Nominal"}},
		{"Sec.P14", {"Character", "Nominal"}},
		{"Sec.P15", {"Character", "Nominal"}},
		{"Sec.P16", {"Character", "Nominal"}},
		{"Sec.P17", {"Character","Nominal"}},
		{"Sec.P18", {"Character", "Nominal"}},
		{"Sec.P19", {"Character", "Nominal"}},
		{"Sec.P2", {"Character", "Nominal"}},
		{"Sec.P20", {"Character", "Nominal"}},
		{"Sec.P21", {"Character", "Nominal"}},
		{"Sec.P22", {"Character", "Nominal"}},
		{"Sec.P23", {"Character", "Nominal"}},
		{"Sec.P24", {"Character", "Nominal"}},
		{"Sec.P25", {"Character", "Nominal"}},
		{"Sec.P26", {"Character", "Nominal"}},
		{"Sec.P27", {"Character","Nominal"}},
		{"Sec.P3", {"Character", "Nominal"}},
		{"Sec.P4", {"Character", "Nominal"}},
		{"Sec.P5", {"Character", "Nominal"}},
		{"Sec.P6", {"Character", "Nominal"}},
		{"Sec.P7", {"Character", "Nominal"}},
		{"Sec.P8", {"Character", "Nominal"}},
		{"Sec.P9", {"Character", "Nominal"}},
		{"Type", {"Character", "Nominal"}},
		{"S. ID", {"Character", "Nominal"}}
	}
);
 
obtainFileInfo = Function({filePath}, {pathBreak,fileName,fileType,dt,headerStart,indexPieces,indexFileName,serialNum,fileInformation},
	/*
	filePath = files[j];
	*/
	fileInfo = associative array();
	
	pathBreak = Words(filePath,"/");
	fileName = pathBreak[nitems(pathBreak)];
	fileType = Word(nitems(Words(fileName,".")),fileName,".");
	fileNameNoExt = Munger(fileName,1,"."||fileType,"");
	
	//do i need xlsx stuff? the pick file forces to CSV only...
	Match(fileType,
		"xlsx",
		dt = Open(filePath, 
			invisible, //not sure why this working and private not
			//private,
			worksheetsettings(
				1,
				HasColumnHeaders( 0 ), 
				//Headers start on row(colNameStart),
				data starts on row( 1 ),
				data starts on column( 1 ),
				data ends on row( 50 ),
				data ends on column( 15 )
			)
		)
		,
		"csv",
		dt = Open(filePath, 
		   Scan Whole File( 0 ),
		   Labels( 0 ), 
		   Lines To Read( 50 ),///may need to add more as more and more parameters, etc are in file (updated from 15 to 50)
		   Data Starts( 1 ),
		   private
		);
	);
	for(i=1,i<=nrow(dt),i++,
		If(Contains(dt[i,1],"Measure")>0,
			headerStart = i;
			break();
		)
	);
	//may need to update for DXC700
	modelFam = If(
		dt[1,1] == "Index", "DxC700"
		,
		Contains(dt[headerStart,0],"CONC")> 0, "680"
		,
		Contains(dt[headerStart,0],"Conc.")> 0, "5800"
	);
	
	If( Contains( modelFam, "68" ) > 0,
		//rowNum = 2;
		//colNum = 2;
		indexPieces = Words(dt[2,2],":");
		indexFileName = Trim(Concat(indexPieces[1],":",indexPieces[2]),Both);
		serialNum = "";
		,
		modelFam == "5800",
		//rowNum = 1;
		//colNum = 1;
		indexPieces = Words(dt[1,1],":");
		indexFileName = Trim(Concat(indexPieces[1],":",indexPieces[2]),Both);
		serialNum = dt[2,2]
		,
		modelFam == "DxC700",
		indexPieces = Words(dt[1,2],":");
		indexFileName = Trim(ConcatItems(indexPieces,":"),Both);
		serialNum = dt[2,2];
	);
	//above the indexFileName could be removed from the if() by using concatItems for all...
	fileInformation = EvalExpr(List(List(Expr(IndexFileName),Expr(headerStart),Expr(modelFam),Expr(serialNum),Expr(fileType),Expr(fileNameNoExt))));

	InsertInto(fileInfo, fileName, fileInformation);
	
	Close( dt, nosave );
	Return( fileInfo );
);

openCSV = Function({filepath,colNameStart},
	/*
	filePath = files[j];
	colNameStart = fileInfo[fileName][2];
	*/
		
	dt = Open(filepath,
		Import Settings(
			End Of Line( CRLF, CR, LF ),
			End Of Field( Tab, Comma, CSV( 0 ) ),
			Strip Quotes( 1 ),
			Use Apostrophe as Quotation Mark( 0 ),
			Use Regional Settings( 0 ),
			Scan Whole File( 1 ),
			Treat empty columns as numeric( 0 ),
			CompressNumericColumns( 0 ),
			CompressCharacterColumns( 0 ),
			CompressAllowListCheck( 0 ),
			Labels( 1 ),
			Column Names Start( colNameStart ),
			Data Starts( colNameStart + 1 ),
			Lines To Read( "All" ),
			Year Rule( "20xx" )
		),
		private
	);
	Return(dt);

);

errorList = associative array(
	{
		{"d", "Excluded from QC by user"}, 
		{"e", "Data edited by user."}, 
		{"(", "Shortage of wash solution for contamination parameters."}, 
		{"Wa", "Result has been analyzed with an erroneous cuvette."}, 
		{"R", "Insufficient reagent."}, 
		{"#", "Insufficient sample."}, 
		{"%", "Clot detected."}, 
		{"?", "Unable to calculate a result."}, 
		{"?a", "Sample/Reagent detection abnormal"}, 
		{"n", "LIH test not performed."}, 
		{"l", "Result may be affected by lipemia."}, 
		{"i", "Result may be affected by icterus."}, 
		{"h", "Result may be affected by hemolysis."}, 
		{"Y", "Reagent blank OD at last photometric point high."}, 
		{"U", "Reagent blank OD at last photometric point low."}, 
		{"y", "Reagent blank/routine OD at first photometric point high."}, 
		{"u", "Reagent blank/routine OD at first photometric point low."}, 
		{"@", "OD is higher than 3.0."}, 
		{"$", "Not enough data to determine linearity of reaction."}, 
		{"D", "OD of reaction is higher than maximum OD range."},
		{"B", "OD of reaction is lower than minimum OD range."}, 
		{"*", "Linearity error in rate method."}, 
		{"&", "Prozone test data is abnormal."}, 
		{"Z", "Prozone error."}, 
		{"E", "Overreaction in a rate assay detected."}, 
		{"Fx", "Result (OD) is higher than the dynamic range."}, 
		{"Gx", "Result (OD) is lower than the dynamic range."}, 
		{"!", "Unable to calculate concentration."}, 
		{")", "Reagent lot no. used at sample analysis is different from that used at calibration analysis."}, 
		{"a", "Reagent expired."}, 
		{"ba", "Calibration expired."}, 
		{"bh", "No valid calibration used."}, 
		{"bn", "Mastercurve used."}, 
		{"bz", "Calibration curve for Prozone data used."}, 
		{"F", "Result is higher than the dynamic range."}, 
		{"G", "Result is lower than the dynamic range."}, 
		{"ph", "Result is higher than the upper panic value."},
		{"pl", "Result is lower than the low panic value."}, 
		{"T", "Abnormality found in inter-chemistry check."}, 
		{"P", "Positive"}, 
		{"N", "Negative"}, 
		{"H", "Result is higher than reference range."}, 
		{"L", "Result is lower than reference range."}, 
		{"J", "Result is higher than the repeat decision range."},
		{"K", "Result is lower than the repeat decision range."}, 
		{"fh", "Result is higher than the repeat run reflex range."}, 
		{"fl", "Result is lower than the repeat run reflex range."}, 
		{"Va", "The result of multiple measurement alienation check is NG."}, 
		{"xQ", "Failure of one control used in a multirule QC."}, 
		{"1Q", "QC data exceeds the range entered in the Single Check Level field."}, 
		{"2Q", "QC data exceeds 13SD control range."}, 
		{"3Q", "QC data continuously exceeds the 2 SD control limit."}, 
		{"4Q", "QC data exceeds R4S control range."},
		{"5Q", "QC data exceeds 41S control range."}, 
		{"6Q", "A preset number of consecutive QC results fall on one side of the mean."}, 
		{"7Q", "Consecutive QC results show steadily increasing or decreasing values."}, 
		{"S", "Sample repeated and original results replaced by repeat result."},
		{"/", "Test pending or not analyzed."}, 
		{"r", "Data transmitted to host."}, 
		{"c", "Data corrected by user."}
	}
);

wrongVer = Function( {maxVer},
	{Default Local},
	New Window( "Requires JMP " || Char( maxVer ),
		<<Modal,
		Text Box( "This addin requires the used of JMP " || Char( maxVer ) || "+" ),
		Button Box( "OK" )
	)
);

beckmanCleanup = Function({tableName,fileName,failPos,failList},{j,wholeList,i,a},
	//list = {Data Table( "AU_20200110_1007_RE_000" )};
	//iteration = 1;
	//fileName = "AU_20200110_1007_RE_000.csv";
	/*
	tableName = wholeList[j];
	fileName = fileName;
	failPos = failPosList;
	failList = failListList;
	*/
	
	//j=1;
	//fileType = "csv";
	//pulled in work from both models Trys will change columns that will change
		//orig from 680
	Try(Column(tableName,"CONC") << Set Name("Conc."));
	Try(Column(tableName,"Conc.")<<Data Type("Numeric"));
	Try(Column(tableName,"Conc.")<<Modeling Type("Continuous"));
		//orig from 5800
	Try(Column(tableName,"Measure Time") << Set Name("Measured Date/Time"));
	Try(Column(tableName,"S. No.") << Set Name("Sample No.") << Data Type("Character"));
	Try(Column(tableName,"Cal/QC No.") << Set Name("QC/CalNo."));
	Try(Column(tableName,"Data Flags") << Set Name("DataFlag"));
	Try(Column(tableName,"Conc.")<<Data Type("Numeric"));
	Try(Column(tableName,"Conc.")<<Modeling Type("Continuous"));
		//orig from DxC700
	Try(Column(tableName,"Flags") << Set Name("DataFlag"));
	///// Need add Unit number for DxC data... AND 680 data??
	Try(IsScriptable(tableName:"Unit No."n);
		,
		tableName << new column("Unit No.", Numeric, << set each value(1))
	);
	//CSV data seems to come in "better" than this code suggests it should... it may be a version related thing
	//// TRY THIS IN JMP 14!
	//// May also happen if an xlsx is resaved as a csv (rare, don't plan for?)
	//fileinfo not included as parameter, maybe should do that?
	If(fileInfo[fileName][5] == "csv" & (Column(tableName,1)<<get data type) == "Character",
		tableName << Selectwhere(IsMissing(Num(:Name("Measured Date/Time"))))<<deleterows;
		For(i=1,i<=ncol(tableName),i++,
			a = column(tableName,i) << get name;
			Column(tableName,i) << Set Data Type(beckmanColumnArray[a][1]);
			Column(tableName,i) << Set Modeling Type(beckmanColumnArray[a][2]);
		)
		,
		fileInfo[fileName][5] == "xlsx" & (Column(tableName,1)<<get data type) == "Character",
		tableName << SelectAllRows << deleterows;

		InsertInto(failList,tableName<<getname);
		//InsertInto(failPos,j)
		,
		fileInfo[fileName][5] == "xlsx" & (Column(tableName,1)<<get data type) == "Numeric",
		tableName << Selectwhere(IsMissing(:Name("Measured Date/Time"))) << deleterows;
		,
		fileInfo[fileName][5] == "csv" & (Column(tableName,1)<<get data type) == "Numeric",
		tableName << Selectwhere(IsMissing(:Name("Measured Date/Time"))) << deleterows;
		,
		Empty()
	);
	
	Try(Column(tableName, "Measured Date/Time") << Format("m/d/y h:m:s"));

	Try(colList = tableName << get column names(string);
		flagPos = Contains(colList,"DataFlag");
		emptyCol = Column(tableName,flagPos + 1) << GetName();
		If(Word(1,emptyCol) == "Column", Column(tableName,flagPos + 1) << Set Name("Wavelength"));
		For(i=0,i<=27,i++,
			colRef = "P"||Char(i);
			Column(tableName,colRef)<<Data Type("Numeric");
			Column(tableName,colRef)<<Modeling Type("Continuous");
			Column(tableName,colRef)<<SetName("Rea."||colRef);
		);
		tableName << SelectWhere(:Wavelength == "Reaction OD" | :Wavelength == "Reagent Blank")<<deleterows();
	);

	tableName << Sort(
		By( 
			:Name( "Measured Date/Time" ),
			:Sample No.,
			:Test Name,
			:Name( "QC/CalNo." ),
			:Counts
		),
		Order( 
			Ascending,
			Ascending,
			Ascending,
			Ascending,
			Ascending
		),
		Replace Table( 1 )
	);
	
	//Assumes Data is internal
	tableName << New Column( "Lab", Character( 10 ), 
		<< Set Each Value("IDX-Internal")
	);
	tableName << New Column( "Index File", Character( 10 ), 
		<< Set Each Value(fileInfo[filename][1])
	);
	tableName << New Column( "Model Family", Character( 10 ), 
		<< Set Each Value(fileInfo[filename][3])
	);	
	tableName << New Column( "Serial Number", Character( 10 ), 
		<< Set Each Value(fileInfo[filename][4])
	);

	//680s don't seem to have wavelength??
	tableName << New Column( "UniqueCurveID",Character( 40 ), 
		Formula( :Lab || "_" || :Index File || "_" || Char( :Name("Measured Date/Time") ) ||"_" || Char( :Unit No. ) || "_" || :Test Name || "_" || :Wavelength )
	);
	Column( tableName, "UniqueCurveID" ) << evalFormula << delete formula;

	tableName << New Column( "UniqueRunID",Character( 40 ), 
		Formula( :Lab || "_" || :Index File || "_" || Char( :Name("Measured Date/Time") ) ||"_" || Char( :Unit No. ) || "_" || :Test Name )
	);
	Column( tableName, "UniqueRunID" ) << evalFormula << delete formula;

	tableName << New Column( "Flag Desc",
		Character( 10 ),
		Formula(
			dataList = Words( :DataFlag, " " );
			errorDesc = {};
			For( i = 1, i <= N Items( dataList ), i++,
				Try(InsertInto(errorDesc,errorList[dataList[i]]))
			);
			//show(row(),errorDesc);
			If( errorDesc == {},
				"None",
				Concat Items( errorDesc, "," )
			);
		)
	);
	Column( tableName, "Flag Desc" ) << evalFormula << deleteFormula;//did to fix cases where first couple rows didn't get 'none'
	
	tableName << New Column( "Date", Character( 10 ), Formula( Short Date( :Name( "Measured Date/Time" ) ) ) );
	Column( tableName, "Date" ) << delete formula;

);

failedImport = Function({failTables},{failList},
	failList = failTables;
	Print("These files failed import:");
	For(i=1,i<=nitems(failList),i++,
		Print(failList[i]);
		Close(DataTable(failList[i]),nosave);
	);
	New Window( "Failed Import",
		<<Modal,
		Text Box( "Some files failed import", <<setfontstyle( "bold" ), <<setfontsize( 14 ) ),
		Text Box( "Please see a list in the log: View>Log", <<setfontstyle( "bold" ), <<setfontsize( 12 )),
		Text Box( "Please see Nathan Clark if you have questions on the failures", <<setfontstyle( "bold" ),<<setfontsize( 12 ) ),
		Button Box( "OK" ),
	)
);

fileProcessing = Function({files},{default local},
	/*
	files = beckFiles;
	*/
	wholeList = {};
	failPosList = {};
	failListList = {};
	
	For(j=1, j<= nitems(files),j++,
		pathBreak = Words(files[j],"/");
		fileName = pathBreak[nitems(pathBreak)];
		//fileType = Word(nitems(Words(fileName,".")),fileName,".");
		//fileNameNoExt = Munger(fileName,1,"."||fileType,"");
		//show(fileName,fileType);
		
		message = fileProcessingMsg || Char(j) || " of " || Char(nitems(files));
		newCap(message,1);

		fileInfo = obtainFileInfo(files[j]);
		
		//need to put back that match, and build the open XLS function
		wholeList[j] = openCSV(files[j],fileInfo[fileName][2]); //used to have a match for diff file types
		wholeList[j] << SetName(fileInfo[fileName][6]);
		//j=8;
		//filetype="xlsx";
		beckmanCleanup(wholeList[j],fileName,failPosList,failListList);
		//Print( fileName||" 3d");
		
		wholeList[j] << Move Selected Columns(
			{
				"Index File",
				"UniqueRunID",
				"UniqueCurveID",
				"Lab",
				"Model Family",
				"Serial Number",
				"Date",
				//"Run Type",
				//"Reaction",
				//"Program",
				"Test Name",
				//"Run ID",
				//"Reaction ID", 
				"Unit No.", 
				//"Cal ID",
				//"Cal Used",
				//"Cal Info",
				//"Reaction OD",
				//"AR_Rxn-Blank",
				//"AR_Rxn",
				"Conc."},
			To First
		);
		wholeList[j] << Move Selected Columns(
			{
				"Flag Desc"//,
				//"Cal ID_DataFlag"
			},
			After( "DataFlag" )
		);

	);
	
	///turn this into a foreach or an array removal rather than position
	//RemoveFrom(wholeList,failPos);

	If(nitems(wholeList) == 0,
		
		//If(nitems(failList)>0,
			//Print("These files failed import:");
			//For(i=1,i<=nitems(failList),i++,
			//	Print(failList[i])
			//);
			//failedImport(failList);
			//Close(wholeList[1],nosave);
			//Throw();
		,
		nitems(wholeList) == 1,
		concatTable = wholeList[1] << Subset(
			Copy Formula ( 1 ),
			Suppress formula evaluation( 0 ),
			All rows,
			Selected columns only( 0 ),
			Output Table( "Beckman_"||nextTodayDate||"_splitProgCurves" )
		);
		//tblName = wholeList[1] << getname;
		//concatTable << New Column("Source Table",Formula(tblName));
		//Column(concatTable,"Source Table") << delete formula;
		,
		startConcat = wholeList[1];
		RemoveFrom(wholeList,1);
		concatTable = startConcat << Concatenate(wholeList,Output Table( "Beckman_"||nextTodayDate||"_splitProgCurves" ))//,Create Source Column);
	);

	For(i=1,i<=nitems(wholeList),i++,
		Close(wholeList[i],nosave)
	);
	Try(Close(startConcat,nosave));
	
	Return(concatTable);

);

finalProcessing = Function({tableName},{default local},
	/*
	tableName = concatTable;
	*/
	concatTable = tableName;
	
	message = "Finishing Data Processing";
	newCap(message,1);

	stackCols = {};
	For(j=ncol(concatTable), j>=1,j--,
		a = Column(concatTable,j)<<getname;
		If( Contains(a, "Sec."),
			concatTable << delete columns(j);
		)
	);
	For(i=1, i<=ncol(concatTable),i++,
		a = Column(concatTable,i)<<getname;
		If( Contains(a, "Rea.") | Contains(a, "Pre.") | Contains(a, "RB."),
			Column(concatTable,a)<<data type("numeric");
			Column(concatTable,a)<<modeling type("continuous");
			InsertInto(stackCols, i)
		)
	);
	stackTable = concatTable << Stack(
		columns(
			stackCols
		),
		Source Label Column( "CurveData" ),
		Stacked Data Column( "OD" )
	);

	Column(stackTable,"OD") <<data type("numeric");
	Column(stackTable,"OD") <<modeling type("continuous");

	stackTable << New Column("Curve Source",Character( 10 ), 
		Formula(
			Word(1,:CurveData, ".")
		)
	);
	Column(stackTable,"Curve Source") << Delete formula;

	stackTable << New Column("BeckmanTimePoint",
		Formula(
			Num(Word(2,:CurveData,"P"))
		)
	);
	Column(stackTable,"BeckmanTimePoint") << Delete formula;


	stackTable << New Column("DevTimePoint",
		Formula(
			If(:BeckmanTimePoint == 0, -1, :BeckmanTimePoint)
		)
	);
	Column(stackTable,"DevTimePoint") << Delete formula;

	stackTable << SetName("Beckman_"||nextTodayDate||"_progCurves");

	uniqueCol = "UniqueCurveID";
	parseUnique = EvalInsert(":Name(\!"^uniqueCol^\!")");
	Eval(
		EvalExpr(
			stackTable << New Script("Make justResults",
				tblName = currentdatatable()<<Getname;
				Try(newName = Munger(tblName,1,"_progCurves",""),newName = tblName);
				datatable(tblName) << SelectWhere(Expr(Parse(parseUnique)) != Lag(Expr(Parse(parseUnique)),1));
				datatable(tblName) << Subset(
					output table name( newName|| "_justResults" )
				);
				datatable(tblName) << select all rows;
				datatable(tblName) << invert row selection;
			)
		)
	);
	stackTable << Select Where( :UniqueCurveID != Lag(:UniqueCurveID,1));

	jrTable = stackTable << Subset(
		Selected Rows( 1 ),
		Selected columns only( 0 )
	);

	jrTable << SetName("Beckman_"||nextTodayDate||"_justResults");

	stackTable << Clear Select;

	colorList = {"Index File","UniqueRunID","UniqueCurveID","Lab","Model Family","Serial Number","Date","Flag Desc","Curve Source","BeckmanTimePoint","DevTimePoint"};
	For(i=1,i<=ncol(concatTable),i++,
		If(contains(colorList, Column(concatTable,i) << get name) > 0,
			Column(concatTable, i) << Color Cells(RGBColor(103,255,247));
		)
	);
	For(i=1,i<=ncol(stackTable),i++,
		If(contains(colorList, Column(stackTable,i) << get name) > 0,
			Column(stackTable, i) << Color Cells(RGBColor(103,255,247));
		)
	);
	For(i=1,i<=ncol(jrTable),i++,
		If(contains(colorList, Column(jrTable,i) << get name) > 0,
			Column(jrTable, i) << Color Cells(RGBColor(103,255,247));
		)
	);

	Caption(Remove);

	If(nitems(failList)>0,
		Print("These files failed import:");
		For(i=1,i<=nitems(failList),i++,
			Print(failList[i])
		);
		failedImport();
	);
	
	Return(concatTable,stackTable,jrTable);
);

////////////////////////////////////////
//                                    //
//              Main                  //
//                                    //
////////////////////////////////////////

If( Length( Include File List() ) == 1,

	batchUpdateChk(updateIDs);
	trackUsage(u,addinID);

	If( checkVersion( minJMPver ) == 1,

		beckFiles = Pick File(
		   ,
		   ,
		   {"CSV Files|csv;"},
		   1,
		   0,
		   ,
		   Multiple
		);
		
		If( nitems(beckFiles) == 0,
			Stop();
			,
			splitName = fileProcessing( beckFiles );
			finalProcessing(splitName)
		);
		,
		wrongVer( minJMPver );
		throw();
	)
	,
	Print( "Beckman Processing script was included from another source" );

);